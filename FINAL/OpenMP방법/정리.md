# OpenMP 기반 병렬 블록 해시 조인 알고리즘 정리

## 개요

이 문서는 OpenMP를 활용한 병렬 블록 해시 조인 알고리즘의 구현을 정리한 문서입니다. 독립 스캔 아키텍처를 기반으로 각 스레드가 독립적으로 파일 I/O를 수행하며, Customer-Order 조인을 수행합니다.

## 아키텍처 특징

### 독립 스캔 아키텍처 (Independent Scan Architecture)
- **각 스레드 독립적 I/O**: 각 스레드가 독립적으로 파일을 열고 읽음
- **Customer 데이터 분할**: 총 Customer 레코드를 스레드 수만큼 균등 분할
- **Order 데이터 중복 스캔**: 각 스레드가 전체 Order 파일을 독립적으로 스캔
- **해시 기반 조인**: O(1) 탐색을 위한 해시 테이블 사용

### 장단점
- **장점**: 구현이 단순, 스레드 간 동기화 불필요
- **단점**: Order 파일 I/O 중복 발생 (스레드 수만큼 중복 읽기)

## 전체 플로우

```
1. 초기화 단계
   ├── 출력 파일 초기화 (disk_save_init)
   ├── Customer 파일 크기 계산
   └── 작업 범위 분배 (chunk_size 계산)

2. OpenMP 병렬 처리
   ├── 각 스레드 초기화
   │   ├── 파일 리더 생성 (disk_reader_open)
   │   ├── 메모리 버퍼 할당
   │   ├── 해시 테이블 생성
   │   └── 결과 버퍼 생성 (result_buffer_create)
   │
   ├── Customer 파트 처리 루프
   │   ├── start_line까지 스킵
   │   ├── 블록 단위 Customer 읽기
   │   ├── 해시 테이블 구축
   │   ├── Order 전체 스캔 및 조인
   │   └── 해시 테이블 정리
   │
   └── 리소스 정리
       ├── 결과 버퍼 정리 (result_buffer_destroy)
       └── 파일 리더 정리 (disk_reader_close)

3. 최종 마무리
   └── 출력 파일 통계 추가 (disk_save_finalize)
```

## 주요 함수 상세 설명

### 1. disk_parallel_block_nested_loop_join_hash_save()

**목적**: 메인 조인 함수, 전체 알고리즘 오케스트레이션

**매개변수**:
- `customer_file`: Customer 데이터 파일 경로
- `order_file`: Order 데이터 파일 경로
- `buffer_blocks`: I/O 버퍼 블록 수
- `output_file`: 결과 저장 파일 경로
- `num_threads`: 사용할 스레드 수

**동작 플로우**:
1. 출력 파일 초기화
2. Customer 파일 총 라인 수 계산 (작업 분배용)
3. OpenMP 병렬 영역 시작 (`#pragma omp parallel for`)
4. 각 스레드별 작업 수행
5. 결과 합산 및 파일 마무리

### 2. DiskReader 관련 함수들

#### disk_reader_open()
**목적**: 파일을 열고 DiskReader 구조체 초기화
**반환**: DiskReader 포인터 또는 NULL (실패 시)

#### disk_reader_read_customer() / disk_reader_read_order()
**목적**: 파일에서 다음 레코드를 읽어 구조체에 저장
**반환**: 성공 시 1, EOF 또는 오류 시 0

#### disk_reader_reset()
**목적**: 파일 포인터를 처음으로 리셋 (Order 재스캔용)
**동작**: 파일을 닫고 다시 열어 포인터 초기화

#### disk_reader_close()
**목적**: 파일 리더 리소스 해제
**동작**: 파일 포인터 닫고 메모리 해제

#### disk_reader_get_io_count()
**목적**: 현재까지 수행된 I/O 블록 수 반환
**용도**: 버퍼 크기 제한 체크

### 3. ResultBuffer 관련 함수들

#### result_buffer_create()
**목적**: 결과 저장용 버퍼 생성
**매개변수**:
- `output_file`: 출력 파일 경로
- `buffer_size`: 버퍼 크기 (레코드 수)
**동작**: 파일 열기 및 버퍼 메모리 할당

#### result_buffer_add()
**목적**: 매칭된 Customer-Order 쌍을 버퍼에 추가
**매개변수**:
- `buffer`: ResultBuffer 포인터
- `customer`: CustomerRecord 포인터
- `order`: OrderRecord 포인터
**동작**: 버퍼에 저장, 꽉 차면 파일에 플러시

#### result_buffer_destroy()
**목적**: 버퍼 리소스 해제 및 남은 데이터 플러시
**동작**: 버퍼에 남은 데이터 파일에 쓰고 메모리 해제

### 4. disk_save 관련 함수들

#### disk_save_init()
**목적**: 출력 파일 초기화
**동작**: 파일 생성 및 헤더 작성

#### disk_save_finalize()
**목적**: 출력 파일 마무리
**동작**: 통계 정보 추가 및 파일 닫기

## 알고리즘 상세 동작

### 블록 단위 처리 방식
```
While (현재 라인 < 끝 라인):
    1. Customer 블록 읽기
       - 최대 max_records 또는 buffer_blocks I/O까지 읽기
       - 메모리 버퍼에 저장

    2. 해시 테이블 구축
       - Customer 키를 해시하여 O(1) 탐색 준비
       - 충돌 시 체이닝으로 연결

    3. Order 전체 스캔
       While (Order 파일 끝까지):
           - Order 블록 읽기
           - 각 Order에 대해 해시 테이블 탐색
           - 매칭 시 결과 버퍼에 저장

    4. 해시 테이블 정리
       - 메모리 누수 방지 위해 완전 해제
```

### 메모리 관리 전략
- **블록 버퍼링**: 한 번에 많은 데이터를 메모리에 로드하지 않음
- **해시 테이블**: 블록 단위로 생성/파괴하여 메모리 사용 최적화
- **결과 버퍼링**: 10,000개 단위로 디스크에 플러시

### I/O 최적화 기법
- **블록 단위 읽기**: buffer_blocks 파라미터로 I/O 크기 제어
- **버퍼링**: 메모리 버퍼를 통한 효율적 데이터 처리
- **스트리밍**: 파일을 순차적으로 읽어 캐시 효율성 향상

## 성능 특징

### 측정 결과 (2스레드, buffer_blocks=100)
- **실행 시간**: 2.49초
- **총 I/O 횟수**: 131회
- **매칭 레코드**: 1,500,000개
- **I/O 중복**: Order 파일을 2번 읽음 (스레드 수만큼)

### 스케일링 특성
- **CPU 스케일링**: 스레드 수 증가 시 CPU 처리 시간 감소
- **I/O 병목**: Order 중복 읽기로 인한 I/O 오버헤드
- **메모리 사용**: 스레드 수에 비례하여 증가

## 개선 가능 방향

### 1. 공유 스캔 아키텍처 도입
- 단일 스레드가 Order 읽기, 다른 스레드들이 공유
- I/O 중복 제거로 성능 향상
- 동기화 복잡도 증가

### 2. 파티셔닝 최적화
- Customer 키 기반 파티셔닝으로 Order도 분할
- 각 스레드가 관련 Order 파트만 처리
- I/O 및 메모리 효율성 향상

### 3. 메모리 최적화
- 해시 테이블 크기 동적 조정
- 더 큰 블록 크기로 I/O 효율성 향상

## 결론

현재 구현은 OpenMP의 간단함을 활용한 독립 스캔 아키텍처로, 구현이 쉽고 디버깅이 용이합니다. 다만 Order I/O 중복이라는 단점이 있으며, 대규모 데이터에서는 공유 스캔 아키텍처로의 개선이 필요할 수 있습니다.

이 알고리즘은 교육용으로도 적합하며, 병렬 프로그래밍의 기본 개념(작업 분배, 동기화, 메모리 관리)을 잘 보여줍니다.